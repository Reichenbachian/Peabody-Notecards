{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checking via Text Prediction Exploration\n",
    "## Nicholas Miklaucic & Peabody Work Duty Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spellcheck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from string import punctuation, whitespace\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A blog is nice this time of day.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.sentence_correct(\"A blob is nice this timem of day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name urude quartzite stone.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.sentence_correct(\"Name urude quartzite stone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unwanted_chars = list(punctuation) + list(whitespace)\n",
    "unwanted_chars.remove(' ')\n",
    "unwanted_chars.remove('-')\n",
    "unwanted_chars.remove(\"'\")\n",
    "def word_parse(string):\n",
    "    \"\"\"Lowercases, emoves punctutation besides that which can appear in the inside of words (hyphen, apostrophe), and removes extraneous whitespace.\"\"\"\n",
    "    parsed = string.strip().lower()\n",
    "    for unwanted_char in unwanted_chars:\n",
    "        parsed = parsed.replace(unwanted_char, '')\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Cat Number</th>\n",
       "      <th>Site Number</th>\n",
       "      <th>Locality</th>\n",
       "      <th>Site</th>\n",
       "      <th>Name</th>\n",
       "      <th>Situation</th>\n",
       "      <th>AccNum</th>\n",
       "      <th>fileLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M50/1</td>\n",
       "      <td>Locality Squibnocket Head, so Martha' 3 Vineya...</td>\n",
       "      <td>Site Squibnocket Cliff .</td>\n",
       "      <td>Name Butt of arrowhead.</td>\n",
       "      <td>Situation on sand under shell just south of st...</td>\n",
       "      <td>1</td>\n",
       "      <td>peabody_files/Accession Files/1/1_0001.pdf.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>M50/1</td>\n",
       "      <td>Lmnmw Squibnocket Head, so Martha's Vineyard, M</td>\n",
       "      <td>Site Squibnocket Cliff.</td>\n",
       "      <td>Name Butt of quartz knife.</td>\n",
       "      <td>Situation Black sandy loam near stake 2.</td>\n",
       "      <td>1</td>\n",
       "      <td>peabody_files/Accession Files/1/1_0002.pdf.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>M50/1</td>\n",
       "      <td>Locality Squibnocket Head, sou Martha' 3 Viney...</td>\n",
       "      <td>Site Squibnocket Cliff .</td>\n",
       "      <td>Name Crude quartz point.</td>\n",
       "      <td>Situation Black sandy loam, 1M. east of stake A.</td>\n",
       "      <td>1</td>\n",
       "      <td>peabody_files/Accession Files/1/1_0003.pdf.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>M50/1</td>\n",
       "      <td>Locality Squibnooket Head, so Martha's Vineyar...</td>\n",
       "      <td>Site Squibnocket Cliff.</td>\n",
       "      <td>Name Urude quartzite apea:</td>\n",
       "      <td>Situation Top of first shell layer.</td>\n",
       "      <td>1</td>\n",
       "      <td>peabody_files/Accession Files/1/1_0004.pdf.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>M50/1</td>\n",
       "      <td>Locality Squibnocket Head, sou Martha' 3 Viney...</td>\n",
       "      <td>Site Squibnocket Cliff .</td>\n",
       "      <td>Name Crude chopper.</td>\n",
       "      <td>Situation Underneath lowest shell layer.</td>\n",
       "      <td>1</td>\n",
       "      <td>peabody_files/Accession Files/1/1_0005.pdf.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Cat Number Site Number  \\\n",
       "0           0           1       M50/1   \n",
       "1           1           2       M50/1   \n",
       "2           2           3       M50/1   \n",
       "3           3           4       M50/1   \n",
       "4           4           5       M50/1   \n",
       "\n",
       "                                            Locality  \\\n",
       "0  Locality Squibnocket Head, so Martha' 3 Vineya...   \n",
       "1    Lmnmw Squibnocket Head, so Martha's Vineyard, M   \n",
       "2  Locality Squibnocket Head, sou Martha' 3 Viney...   \n",
       "3  Locality Squibnooket Head, so Martha's Vineyar...   \n",
       "4  Locality Squibnocket Head, sou Martha' 3 Viney...   \n",
       "\n",
       "                       Site                        Name  \\\n",
       "0  Site Squibnocket Cliff .     Name Butt of arrowhead.   \n",
       "1   Site Squibnocket Cliff.  Name Butt of quartz knife.   \n",
       "2  Site Squibnocket Cliff .    Name Crude quartz point.   \n",
       "3   Site Squibnocket Cliff.  Name Urude quartzite apea:   \n",
       "4  Site Squibnocket Cliff .         Name Crude chopper.   \n",
       "\n",
       "                                           Situation  AccNum  \\\n",
       "0  Situation on sand under shell just south of st...       1   \n",
       "1           Situation Black sandy loam near stake 2.       1   \n",
       "2   Situation Black sandy loam, 1M. east of stake A.       1   \n",
       "3                Situation Top of first shell layer.       1   \n",
       "4           Situation Underneath lowest shell layer.       1   \n",
       "\n",
       "                                     fileLocation  \n",
       "0  peabody_files/Accession Files/1/1_0001.pdf.png  \n",
       "1  peabody_files/Accession Files/1/1_0002.pdf.png  \n",
       "2  peabody_files/Accession Files/1/1_0003.pdf.png  \n",
       "3  peabody_files/Accession Files/1/1_0004.pdf.png  \n",
       "4  peabody_files/Accession Files/1/1_0005.pdf.png  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_df = pd.read_csv(\"test.csv\")\n",
    "other_df.replace({r'\\n': ' '}, regex=True ,inplace=True)\n",
    "other_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-6-f84b03c0de1e>\", line 13, in correctRow\n",
      "    row[\"Situation\"] = spellcheck.sentence_correct(row[\"Situation\"])\n",
      "  File \"<ipython-input-6-f84b03c0de1e>\", line 13, in correctRow\n",
      "    row[\"Situation\"] = spellcheck.sentence_correct(row[\"Situation\"])\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 91, in sentence_correct\n",
      "    corrected_words.append(''.join(start_punc) + correct(word) + ''.join(end_punc))\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 91, in sentence_correct\n",
      "    corrected_words.append(''.join(start_punc) + correct(word) + ''.join(end_punc))\n",
      "  File \"/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-6-f84b03c0de1e>\", line 13, in correctRow\n",
      "    row[\"Situation\"] = spellcheck.sentence_correct(row[\"Situation\"])\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 38, in correct\n",
      "    while len(tree.query(input_word, curr_distance)) == 0:  # no matches, decrease precision\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 91, in sentence_correct\n",
      "    corrected_words.append(''.join(start_punc) + correct(word) + ''.join(end_punc))\n",
      "  File \"<ipython-input-6-f84b03c0de1e>\", line 13, in correctRow\n",
      "    row[\"Situation\"] = spellcheck.sentence_correct(row[\"Situation\"])\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 42, in correct\n",
      "    matches = tree.query(input_word, curr_distance + PRECISION)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 44, in query\n",
      "    d = self.levenshtein(node[0], target)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 44, in query\n",
      "    d = self.levenshtein(node[0], target)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 114, in levenshtein\n",
      "    current_row[0:-1] + 1)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 91, in sentence_correct\n",
      "    corrected_words.append(''.join(start_punc) + correct(word) + ''.join(end_punc))\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 42, in correct\n",
      "    matches = tree.query(input_word, curr_distance + PRECISION)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 112, in levenshtein\n",
      "    current_row[1:] = np.minimum(\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/spellcheck.py\", line 42, in correct\n",
      "    matches = tree.query(input_word, curr_distance + PRECISION)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 51, in query\n",
      "    new_nodes += [n for n in node[1][low:high] if n is not None]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 44, in query\n",
      "    d = self.levenshtein(node[0], target)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 85, in levenshtein\n",
      "    return BKTree.levenshtein(target, source)\n",
      "  File \"/Users/localhost/Desktop/Projects/Working/PeabodyNotecards/bk_tree.py\", line 114, in levenshtein\n",
      "    current_row[0:-1] + 1)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f84b03c0de1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrectRow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# for index, row in tqdm(other_df.iterrows()):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     print(other_df.loc[index])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/localhost/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fields = [\"Locality\", \"Site\", \"Name\", \"Situation\"]\n",
    "other_df[fields] = other_df[fields].apply(np.vectorize(lambda x: word_parse(str(x))))\n",
    "def correctRow(tup):\n",
    "    i, row = tup\n",
    "    if len(row[\"Locality\"]) > 0:\n",
    "        row[\"Locality\"] = spellcheck.sentence_correct(row[\"Locality\"])\n",
    "    if len(row[\"Site\"]) > 0:\n",
    "        row[\"Site\"] = spellcheck.sentence_correct(row[\"Site\"])\n",
    "    if len(row[\"Name\"]) > 0:\n",
    "        row[\"Name\"] = spellcheck.sentence_correct(row[\"Name\"])\n",
    "    if len(row[\"Situation\"]) > 0:\n",
    "        row[\"Situation\"] = spellcheck.sentence_correct(row[\"Situation\"])\n",
    "    other_df.loc[i] = row\n",
    "rows=list(other_df.iterrows())\n",
    "pool=Pool(processes=29)\n",
    "pool.map(correctRow, rows)\n",
    "pool.close()\n",
    "pool.join()\n",
    "for index, row in tqdm(other_df.iterrows()):\n",
    "    row[\"Locality\"] = row[\"Locality\"].strip()[row[\"Locality\"].index(\" \")+1 if \"Locality\" in row[\"Locality\"] else 0:]\n",
    "    row[\"Name\"] = row[\"Name\"].strip()[row[\"Name\"].index(\" \")+1 if \"Name\" in row[\"Name\"] else 0:]\n",
    "    row[\"Site\"] = row[\"Site\"].strip()[row[\"Site\"].index(\" \")+1 if \"Site\" in row[\"Site\"] else 0:]\n",
    "    row[\"Situation\"] = row[\"Situation\"].strip()[row[\"Situation\"].index(\" \")+1 if \"Situation\" in row[\"Situation\"] else 0:]\n",
    "    other_df.loc[index] = row\n",
    "#     print(other_df.loc[index])\n",
    "\n",
    "#     other_df.loc[index] = correctRow(row)\n",
    "#     print(other_df.loc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "words = Counter()\n",
    "for field in fields:\n",
    "    words.update(Counter(other_df[field].apply(lambda x: x + ' ').sum().strip().split(' ')))\n",
    "for word, freq in words.most_common():\n",
    "    words[word.title()] = words[word]\n",
    "del words['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current idea I have for mixing frequencies is as follows: for any word currently in the corpus, ignore it. Otherwise, set it to a constant times the place in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALPHA = .05 # determines how much normal English words are weighted\n",
    "# for the value, you should probably use the wanted \"top\" value / 10000\n",
    "with open(\"google-10000-english-no-swears.txt\", 'r') as corpusfile:\n",
    "    for i, word in enumerate(reversed(list(corpusfile))):\n",
    "        if word.strip() in words:\n",
    "            continue\n",
    "        else:\n",
    "            words[word.strip()] = int(ALPHA * i)\n",
    "            words[word.strip().title()] = int(ALPHA * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BETA = 200\n",
    "with open(\"name_list.dat\", 'r') as namelistfile:\n",
    "    for line in namelistfile:\n",
    "        processed = line.strip().lower()\n",
    "        if processed in words:\n",
    "            pass\n",
    "        else:\n",
    "            words[processed] = BETA\n",
    "            words[processed.title()] = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del words['i']\n",
    "with open(\"spellcheckcorpus.dat\", 'w') as outputfile:\n",
    "    with open(\"spellcheckcorpuswithfreqs.csv\", 'w') as outputfreqs:\n",
    "        for word, freq in words.most_common():\n",
    "            outputfile.write(word + '\\n')\n",
    "            outputfreqs.write(\"{},{}\\n\".format(word, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# algorithm for computing edit distance\n",
    "# Good artists copy, great artists steal\n",
    "# this is from https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
    "# and I take no credit for it whatsoever\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing examples\n",
    "print(levenshtein(\"history\", \"herstory\"))\n",
    "# should output 2: add 'r', change 'i' to 'e'\n",
    "print(levenshtein(\"t3sstin\", \"testing\"))\n",
    "# should output 3: change '3' to 'e', delete 's', add a 'g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now to spell-check a single word, we find its closest thing in the list of words we have and then settle ties by commonality in the list\n",
    "def correct(input_word):\n",
    "    \"\"\"Returns the closest words in the list of words we have, sorted by likelihood.\"\"\"\n",
    "    candidates = []\n",
    "    curr_min_distance = 200\n",
    "    for word in words:\n",
    "        distance = levenshtein(word, input_word)\n",
    "        if distance < curr_min_distance:\n",
    "            candidates = [word]\n",
    "            curr_min_distance = distance\n",
    "        elif distance == curr_min_distance:\n",
    "            candidates.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    candidates.sort(key=lambda x: words[x], reverse=True)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(correct(\"arrop oint\"))\n",
    "print(correct(\"or1g1nal3fdf\"))\n",
    "print(correct(\"history\"))\n",
    "print(correct(\"mcmurphy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel really good about this system, especially if I ever get the actual English dictionary to back the wordlist up (EDIT: done!) and figure out how to properly mix those. Things to think about improving:\n",
    " * If the OCR collapses one word into many or many words into one, that's really hard for this to catch.\n",
    " * I thought about doing totally next-level optical edit distance stuff, but that seems like overkill.\n",
    " * Knowing when to apply this system is also important: locations and names have to stay that way, and Massachusetts has some *weird* place names that can't be spell-checked and might change constantly.\n",
    " * Word embeddings a la `word2vec` would significantly improve this and I'm working on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The really nifty thing would be to use this algorithm to train a neural network to do deep learning on spell-checking (which has been done successfully), but that REALLY seems like overkill. I'll plug this into the whole pipeline and get the dictionary sorted before I do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
