{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checking via Text Prediction Exploration\n",
    "## Nicholas Miklaucic & Peabody Work Duty Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from string import punctuation, whitespace\n",
    "import spellcheck\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A blog in nice his time of day.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.sentence_correct(\"A blob is nice this timem of day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name crude quartzite stone.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.sentence_correct(\"Name urude quartzite stone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unwanted_chars = list(punctuation) + list(whitespace)\n",
    "unwanted_chars.remove(' ')\n",
    "unwanted_chars.remove('-')\n",
    "unwanted_chars.remove(\"'\")\n",
    "def word_parse(string):\n",
    "    \"\"\"Lowercases, emoves punctutation besides that which can appear in the inside of words (hyphen, apostrophe), and removes extraneous whitespace.\"\"\"\n",
    "    parsed = string.strip().lower()\n",
    "    for unwanted_char in unwanted_chars:\n",
    "        parsed = parsed.replace(unwanted_char, '')\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Locality</th>\n",
       "      <th>Site</th>\n",
       "      <th>Name</th>\n",
       "      <th>Situation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>locality squibnocket head southwest side 0mart...</td>\n",
       "      <td>site squibnocket cliff</td>\n",
       "      <td>name butt of arrowhead</td>\n",
       "      <td>situation on sand under shell just south of st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmnmw squibnocket head southwest side 0martha'...</td>\n",
       "      <td>site squibnocket cliff</td>\n",
       "      <td>name butt of quartz knife</td>\n",
       "      <td>situation black sandy loam near stake 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locality squibnocket head southwest side ofmar...</td>\n",
       "      <td>site squibnocket cliff</td>\n",
       "      <td>name crude quartz point</td>\n",
       "      <td>situation black sandy loam 1m east of stake a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>locality squibnocket head southwest side 0mart...</td>\n",
       "      <td>site squibnocket cliff</td>\n",
       "      <td>name urude quartzite spear</td>\n",
       "      <td>situation top of first shell layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uxﬂky squibnocket head southwest side ofmartha...</td>\n",
       "      <td>site squibnocket cliff</td>\n",
       "      <td>name crude chopper</td>\n",
       "      <td>situation underneath lowest shell layer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Locality                     Site  \\\n",
       "0  locality squibnocket head southwest side 0mart...  site squibnocket cliff    \n",
       "1  lmnmw squibnocket head southwest side 0martha'...   site squibnocket cliff   \n",
       "2  locality squibnocket head southwest side ofmar...  site squibnocket cliff    \n",
       "3  locality squibnocket head southwest side 0mart...   site squibnocket cliff   \n",
       "4  uxﬂky squibnocket head southwest side ofmartha...  site squibnocket cliff    \n",
       "\n",
       "                         Name  \\\n",
       "0      name butt of arrowhead   \n",
       "1   name butt of quartz knife   \n",
       "2     name crude quartz point   \n",
       "3  name urude quartzite spear   \n",
       "4          name crude chopper   \n",
       "\n",
       "                                           Situation  \n",
       "0  situation on sand under shell just south of st...  \n",
       "1            situation black sandy loam near stake 2  \n",
       "2      situation black sandy loam 1m east of stake a  \n",
       "3                 situation top of first shell layer  \n",
       "4            situation underneath lowest shell layer  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "fields = [\"Locality\", \"Site\", \"Name\", \"Situation\"]\n",
    "other_df = other_df[fields]\n",
    "other_df[fields] = other_df[fields].apply(np.vectorize(lambda x: word_parse(str(x))))\n",
    "other_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "for field in fields:\n",
    "    words.update(Counter(other_df[field].apply(lambda x: x + ' ').sum().strip().split(' ')))\n",
    "for word, freq in words.most_common():\n",
    "    words[word.title()] = words[word]\n",
    "del words['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Site', 2955),\n",
       " ('site', 2955),\n",
       " ('Locality', 2726),\n",
       " ('locality', 2726),\n",
       " ('Name', 2672),\n",
       " ('name', 2672),\n",
       " ('situation', 2592),\n",
       " ('Situation', 2592),\n",
       " ('shell', 2319),\n",
       " ('Shell', 2319)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current idea I have for mixing frequencies is as follows: for any word currently in the corpus, ignore it. Otherwise, set it to a constant times the place in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALPHA = .05 # determines how much normal English words are weighted\n",
    "# for the value, you should probably use the wanted \"top\" value / 10000\n",
    "with open(\"google-10000-english-no-swears.txt\", 'r') as corpusfile:\n",
    "    for i, word in enumerate(reversed(list(corpusfile))):\n",
    "        if word.strip() in words:\n",
    "            continue\n",
    "        else:\n",
    "            words[word.strip()] = int(ALPHA * i)\n",
    "            words[word.strip().title()] = int(ALPHA * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Site', 2955),\n",
       " ('site', 2955),\n",
       " ('Locality', 2726),\n",
       " ('locality', 2726),\n",
       " ('name', 2672),\n",
       " ('Name', 2672),\n",
       " ('Situation', 2592),\n",
       " ('situation', 2592),\n",
       " ('shell', 2319),\n",
       " ('Shell', 2319),\n",
       " ('Heap', 1589),\n",
       " ('heap', 1589),\n",
       " ('of', 1576),\n",
       " ('Of', 1576),\n",
       " ('Falls', 1364),\n",
       " ('falls', 1364),\n",
       " ('nevin', 1339),\n",
       " ('Nevin', 1339),\n",
       " ('bluehill', 1181),\n",
       " ('Bluehill', 1181),\n",
       " ('section', 1127),\n",
       " ('Section', 1127),\n",
       " ('Trench', 1111),\n",
       " ('trench', 1111),\n",
       " ('Me', 989),\n",
       " ('me', 989),\n",
       " ('In', 942),\n",
       " ('in', 942),\n",
       " ('east', 502),\n",
       " ('East', 502),\n",
       " ('can', 494),\n",
       " ('Can', 494),\n",
       " ('Home', 494),\n",
       " ('will', 494),\n",
       " ('More', 494),\n",
       " ('home', 494),\n",
       " ('your', 494),\n",
       " ('Your', 494),\n",
       " ('Will', 494),\n",
       " ('more', 494),\n",
       " ('Search', 493),\n",
       " ('time', 493),\n",
       " ('Other', 493),\n",
       " ('other', 493),\n",
       " ('Free', 493),\n",
       " ('Time', 493),\n",
       " ('search', 493),\n",
       " ('Our', 493),\n",
       " ('our', 493),\n",
       " ('free', 493)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BETA = 200\n",
    "with open(\"name_list.dat\", 'r') as namelistfile:\n",
    "    for line in namelistfile:\n",
    "        processed = line.strip().lower()\n",
    "        if processed in words:\n",
    "            pass\n",
    "        else:\n",
    "            words[processed] = BETA\n",
    "            words[processed.title()] = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del words['i']\n",
    "with open(\"spellcheckcorpus.dat\", 'w') as outputfile:\n",
    "    with open(\"spellcheckcorpuswithfreqs.csv\", 'w') as outputfreqs:\n",
    "        for word, freq in words.most_common():\n",
    "            outputfile.write(word + '\\n')\n",
    "            outputfreqs.write(\"{},{}\\n\".format(word, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# algorithm for computing edit distance\n",
    "# Good artists copy, great artists steal\n",
    "# this is from https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
    "# and I take no credit for it whatsoever\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# testing examples\n",
    "print(levenshtein(\"history\", \"herstory\"))\n",
    "# should output 2: add 'r', change 'i' to 'e'\n",
    "print(levenshtein(\"t3sstin\", \"testing\"))\n",
    "# should output 3: change '3' to 'e', delete 's', add a 'g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now to spell-check a single word, we find its closest thing in the list of words we have and then settle ties by commonality in the list\n",
    "def correct(input_word):\n",
    "    \"\"\"Returns the closest words in the list of words we have, sorted by likelihood.\"\"\"\n",
    "    candidates = []\n",
    "    curr_min_distance = 200\n",
    "    for word in words:\n",
    "        distance = levenshtein(word, input_word)\n",
    "        if distance < curr_min_distance:\n",
    "            candidates = [word]\n",
    "            curr_min_distance = distance\n",
    "        elif distance == curr_min_distance:\n",
    "            candidates.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    candidates.sort(key=lambda x: words[x], reverse=True)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arrowpoint']\n",
      "['originally', 'original']\n",
      "['history']\n",
      "['murphy']\n"
     ]
    }
   ],
   "source": [
    "print(correct(\"arrop oint\"))\n",
    "print(correct(\"or1g1nal3fdf\"))\n",
    "print(correct(\"history\"))\n",
    "print(correct(\"mcmurphy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel really good about this system, especially if I ever get the actual English dictionary to back the wordlist up (EDIT: done!) and figure out how to properly mix those. Things to think about improving:\n",
    " * If the OCR collapses one word into many or many words into one, that's really hard for this to catch.\n",
    " * I thought about doing totally next-level optical edit distance stuff, but that seems like overkill.\n",
    " * Knowing when to apply this system is also important: locations and names have to stay that way, and Massachusetts has some *weird* place names that can't be spell-checked and might change constantly.\n",
    " * Word embeddings a la `word2vec` would significantly improve this and I'm working on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The really nifty thing would be to use this algorithm to train a neural network to do deep learning on spell-checking (which has been done successfully), but that REALLY seems like overkill. I'll plug this into the whole pipeline and get the dictionary sorted before I do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
